# TraxxasLLM - Voice-Controlled RC Vehicle System

A sophisticated voice-controlled RC vehicle system that transforms a Traxxas RC car into an intelligent robot capable of understanding and executing natural language commands with real-time audio feedback.

## Project Overview

TraxxasLLM enables natural language interaction with a physical Traxxas RC vehicle through an intelligent voice command pipeline. The system uses an ESP32 microcontroller to control the vehicle's steering and throttle based on commands generated by a large language model. Users can issue conversational commands like "go forward 10 feet" or "turn left and accelerate," which are processed through speech recognition, LLM planning, and precise motor control execution.

The system features two implementation versions:
- **V1**: Local processing with wake-word activation using Ollama LLM and Edge-TTS
- **V2**: Cloud-optimized parallel processing with OpenAI APIs for 2-3 second response times

## Key Features

- **Natural Language Understanding**: Conversational command processing instead of rigid command syntax
- **Precise Motor Control**: PWM-level control of steering and throttle via ESP32 WebSocket connection
- **Real-Time Audio Feedback**: Synthesized speech responses with witty, satirical personality
- **Multiple Interaction Modes**: Wake-word activation, push-to-talk, and continuous listening
- **Parallel Processing (V2)**: Overlapping Whisper, LLM, and TTS operations for sub-3-second response times
- **WebSocket Communication**: Real-time bidirectional command and telemetry streaming
- **Context-Aware Conversations**: Maintains conversation history for intelligent follow-up commands
- **Distance Tracking**: Encoder-based precise distance measurement (416 ticks/foot)

## System Architecture

### High-Level Pipeline

```
Voice Input → Speech-to-Text → LLM Planning → Command Dispatch → ESP32 → Traxxas Motors
     ↓                                                                         ↓
Audio Feedback ←─────────── Text-to-Speech ←─────────────────── ACK/Complete ─┘
```

### Component Stack

**Hardware**
- Traxxas RC vehicle with servo/ESC control
- ESP32 microcontroller running WebSocket client firmware
- Quadrature encoder for distance measurement
- PC/laptop for voice processing and LLM inference

**Software - V1 (Local Processing)**
- FastAPI server with command dispatcher
- Whisper (local) for speech recognition
- Ollama (llama3.1:8b) for command planning
- Edge-TTS for audio synthesis
- WebSocket for ESP32 communication

**Software - V2 (Cloud Optimized)**
- FastAPI server with parallel processing pipeline
- OpenAI Whisper API for transcription
- OpenAI GPT-4o for command planning
- ElevenLabs API for premium TTS
- Async/await architecture for concurrent operations

## Technical Implementation

### Voice Command Flow

1. **Audio Capture**: PyAudio captures microphone input with VAD-based voice activity detection
2. **Speech Recognition**: Whisper transcribes audio to text (local model or API)
3. **LLM Planning**: GPT-4o/Llama generates structured motor commands from natural language
4. **Command Dispatching**: Background async task queues and sends commands to ESP32 via WebSocket
5. **Motor Execution**: ESP32 translates commands to PWM signals for servos and ESC
6. **Feedback Loop**: Completion acknowledgments trigger TTS audio responses

### Command Protocol

Commands are transmitted as JSON over WebSocket:

```json
{
  "type": "command",
  "msg_id": 1,
  "payload": {
    "action": "move_dist",
    "throt": 1800,
    "steer": 1400,
    "feet": 10.0,
    "timeout_ms": 60000
  }
}
```

**Supported Actions**:
- `move_time`: Timed movement with specified throttle/steering
- `move_dist`: Distance-based movement using encoder feedback
- `stop`: Emergency motor kill
- `macro`: Pre-programmed movement sequences

### LLM Response Format

The LLM generates structured responses:

```json
{
  "say": "Ten feet? How specific",
  "steps": [
    {
      "action": "move_dist",
      "throt": 1800,
      "steer": 1400,
      "feet": 10
    }
  ]
}
```

## Performance Characteristics

**V1 (Local Processing)**
- Transcription: ~3-5 seconds (Whisper base.en)
- LLM Planning: ~2-4 seconds (Llama3.1:8b)
- TTS: ~0.5-1 second (Edge-TTS)
- **Total: ~6-10 seconds**

**V2 (Cloud + Parallel)**
- Transcription: ~1-2 seconds (Whisper API)
- LLM Planning: ~0.7-2.5 seconds (GPT-4o)
- TTS: ~0.5-1 second (ElevenLabs)
- **Total: ~2-4 seconds** (overlapped execution)

## Motor Control Specifications

**Steering Range**
- Left: 1000-1399µs
- Center (Neutral): 1400µs (chassis-specific calibration)
- Right: 1401-1800µs

**Throttle Range**
- Reverse: 1200-1499µs
- Neutral: 1500µs
- Forward: 1501-1950µs

**Distance Measurement**
- Encoder: 416 ticks per foot
- Accuracy: ±5% over 20 feet

## Project Structure

```
TraxxasLLM/
├── server.py              # V1 server with local models
├── server_v2.py           # V2 server with cloud APIs and parallel processing
├── voice_assistant.py     # V1 client with wake-word detection
├── voice_assistant_v2.py  # V2 client with push-to-talk
├── llmreciever.ino        # ESP32 firmware for motor control
├── config_v2.py           # Configuration management system
├── requirements_v2.txt    # Python dependencies
└── QUICKSTART_V2.md       # Quick start guide for V2
```

## Quick Start

### Prerequisites

- Python 3.8+
- ESP32 development board
- Traxxas RC vehicle
- Microphone for voice input

### Installation

1. Clone the repository:
```bash
git clone https://github.com/DanVelarde00/TraxxasLLM.git
cd TraxxasLLM
```

2. Install Python dependencies:
```bash
pip install -r requirements_v2.txt
```

3. Configure API keys (for V2):
```bash
# Create .env file
OPENAI_API_KEY=your_openai_key
ELEVENLABS_API_KEY=your_elevenlabs_key
```

4. Flash ESP32 firmware:
- Open `llmreciever.ino` in Arduino IDE
- Configure WiFi credentials and server IP
- Upload to ESP32

### Running V2 (Recommended)

**Server:**
```bash
python server_v2.py
```

**Client:**
```bash
python voice_assistant_v2.py
```

Press and hold 'V' to record commands, release to send.

See [QUICKSTART_V2.md](QUICKSTART_V2.md) for detailed instructions.

## Development Journey & Key Learnings

### Challenge 1: Latency Optimization
**Problem**: Initial implementation had 10-15 second delays between command and execution.

**Solution**: Implemented parallel processing pipeline where Whisper transcription, LLM inference, and TTS generation overlap when possible, reducing total latency by 60%.

### Challenge 2: LLM Precision
**Problem**: Smaller models (3B parameters) produced inconsistent steering values and failed to maintain neutral positions.

**Solution**: Upgraded to Llama3.1:8b locally and GPT-4o for cloud, which reliably generate precise PWM values and handle numerical constraints.

### Challenge 3: Audio Feedback Loops
**Problem**: System would process its own TTS output, creating command loops.

**Solution**: Implemented cooldown periods, audio stream gating, and keyboard debouncing to prevent self-triggering.

### Challenge 4: WebSocket Message Format Compatibility
**Problem**: V2 initially used different field names than ESP32 firmware expected.

**Solution**: Standardized on `msg_id` (int) and `payload` fields to maintain backward compatibility with V1 firmware.

## Technical Highlights

- **Non-blocking ESP32 Execution**: Fully async motor control allows simultaneous command execution and telemetry reporting
- **Command Status Tracking**: ACK/COMPLETE protocol ensures reliable command delivery and execution confirmation
- **Conversation Context**: LLM maintains multi-turn context for natural follow-up commands
- **Error Handling**: Comprehensive timeout and retry logic for network and execution failures
- **Configurable Personality**: System prompt engineering creates witty, satirical robot responses

## Future Enhancements

- Computer vision integration for obstacle detection
- Path planning with A* algorithm
- Multi-vehicle coordination
- Mobile app interface
- Voice biometrics for access control
- Enhanced macro library for complex maneuvers

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs and feature requests.

## License

MIT License - See [LICENSE](LICENSE) for details

## Acknowledgments

- OpenAI for Whisper and GPT-4 APIs
- ElevenLabs for premium TTS
- Ollama for local LLM inference
- FastAPI and Pydantic teams for excellent async frameworks

---

**Author**: Dan Velarde
**Current Status**: Production-ready V2 system with 2-3 second response times
